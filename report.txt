Research Question and Data Specification 

The research question I aim to answer is: "Can I predict the activity being performed based on measurements from the smartphone's accelerometer, linear acceleration, and magnetometer?" These measurements, capturing three-dimensional acceleration and magnetic field, are the primary data I will use to answer this question.
Data Collection, Processing, and Exploratory Analysis
The data collection process was guided by the capabilities of the data collection device and the nature of the research question. Cathleen's phone did not have a gyroscope, and out of all the available measurements in the Phyphox app, the accelerometer, linear acceleration, and magnetometer seemed most suitable. These measurements, capturing three-dimensional acceleration and magnetic field, were chosen for their potential predictive value for our target activities, which include 'labelCycling', 'labelStairs', 'labelWalking', 'labelSitting', and 'labelOther'. Other measurements, such as proximity or light, were deemed to have less predictive value and were therefore not included in the data collection process.
Data was collected using smartphone sensors during a variety of activities. The activities that were recorded during Cathleen's data collection included sitting, walking, going up or down stairs, cycling, and an activity labeled as "looking for bike in circles", which I categorized as "Other". The specific categorization of activities was as follows:
•	Walking: Included labels such as "Walk", "walking", "Walking and standing", "walking + minor stairs up", "walking (app doesn't record when on the phone so a break of the data)", "walking + standing"
•	Stairs: Included labels such as "minor stairs down", "minor stairs up", "minor stairs", "multiple stairs", "6 floors stairs", "stairs down 6 floors", "2 floors stairs up and down consecutively", "2 floors stairs up and down", "stairs up 2 floors"
•	Cycling: Included labels such as "Cycle", "cycling"
•	Sitting: Included labels such as "Sitting (lecture)", "Sitting"
•	Other: Included labels such as "looking for bike in circles"
After collection, the data was processed into a format suitable for analysis. This involved renaming columns, adding sensor type and device type columns, and reordering the columns to align with the desired format.
The data processing resolution was set to 1000ms (or 1 second), a strategic choice guided by the data collection frequency and the objective of striking a balance between precision and computational efficiency. High resolution was desirable to capture subtle variations in sensor readings tied to different activities, but given our data's sampling frequency, a 1000ms resolution was deemed adequate.
As part of our data analysis, a frequency content analysis was performed. This involved computing the Fast Fourier Transform (FFT) of the signal and identifying the frequency at which a significant fraction of the signal’s energy is concentrated. This analysis was instrumental in determining the optimal cutoff frequencies for the application of a subsequent low-pass filter.
To understand the fundamental characteristics of the data, an exploratory data analysis was undertaken. Descriptive statistics were computed for each sensor measurement, providing insights into count, mean, standard deviation, and other salient statistics for each sensor measurement. Additionally, data visualization techniques such as boxplots and line plots were employed to further comprehend the distribution and trends of the data.
Reflecting critically on this stage, the data collection process was constrained by the available sensors on Cathleen's phone, and the data cleaning process required careful consideration of the appropriate resolution and statistical techniques to use. These choices could impact the results of the analysis, as they determine the granularity and quality of the data that is fed into the subsequent stages of the process.
Data Cleaning: Noise Reduction, Missing Values Handling, and Outlier Detection
The data underwent a series of cleaning and preprocessing steps to ensure its quality and suitability for subsequent analysis. The first step involved noise identification and mitigation. Noise in the data was identified using Chauvenet's criterion, a robust outlier detection technique applied to each column in the dataset. This method was chosen for its ability to effectively identify outliers in a distribution without making strong assumptions about the underlying statistical distribution. Chauvenet's criterion was preferred over other outlier detection techniques due to its robustness and suitability for our dataset, which may not necessarily follow a normal distribution. This step was crucial in reducing the impact of noise and outliers on the performance of the subsequent machine learning models.
Missing values in the dataset were another significant concern. A mask was created to locate where NaN values were present. The columns with new NaNs were identified and printed, ensuring that only valid data points were considered in the analysis. This step enhanced the accuracy of the results by ensuring that our models were not trained on incomplete or misleading data.
To further refine the data, missing values were imputed using interpolation. This method was chosen because it provides a more accurate estimate of missing values, especially for time series data. Interpolation can better capture the temporal patterns in the data, which are crucial for activity recognition. This step was instrumental in ensuring that our dataset was complete and ready for analysis.
A significant step in the data cleaning process was the application of a low-pass filter to the accelerometer, linear acceleration, and magnetometer measurements in the x, y, and z directions. The purpose of this step was to reduce the importance of high-frequency noise that is not relevant for activity recognition. The optimal cutoff frequencies, determined from the previous frequency content analysis, were 0.43 Hz for the accelerometer, 0.47 Hz for the linear acceleration, and 0.12 Hz for the magnetometer. This step was crucial in ensuring that our models were trained on the most relevant features of the data.
Finally, Principal Component Analysis (PCA) was applied to the dataset. PCA is a technique used to reduce the dimensionality of the data, and it was applied with 7 components, as this was found to explain most of the variance in the data. This step was instrumental in reducing the complexity of our models, improving their computational efficiency, and potentially improving their performance by reducing the risk of overfitting.
Reflecting critically on this stage, the data cleaning process was a complex task that required careful consideration of the appropriate techniques to use for outlier detection and missing value imputation. The choices made during this process could impact the results of the analysis, as they determine the quality of the data that is fed into the subsequent stages of the process.
Feature Engineering: Numerical, Categorical, and Temporal Data
After cleaning and preprocessing the data, I transitioned to the next stage of the process: feature engineering. This stage involves the creation of new features or the modification of existing features in the dataset to improve machine learning model performance. The goal is to provide the model with the most relevant information in a format that it can understand and learn from. The transition from data cleaning to feature engineering represents a shift from preparing the data to actively shaping it to best serve my research question: "Can I predict the activity being performed based on measurements from the smartphone's accelerometer, linear acceleration, and magnetometer?" This transition is a crucial part of the process, as it sets the stage for the application of machine learning techniques to my cleaned and preprocessed data.
In the context of my research, which involves tracking personal data generated by my own behavioral activities, I have a variety of data types, including numerical, categorical, and temporal data. Each of these data types requires a different approach to feature engineering.
For numerical data, I used a window of time to summarize the data over that window with various measures such as the mean, median, minimum, maximum, standard deviation, or slope. This approach is known as numerical abstraction. By summarizing the data over a window of time, I can capture trends and patterns in the data that might be relevant for prediction tasks. The choice of a window size of 30 was made based on the nature of my data and the activities I was trying to predict. A window size of 30 seconds was deemed sufficient to capture the relevant patterns in the data without introducing too much noise. However, it's worth noting that the optimal window size can vary depending on the specific characteristics of the data and the problem at hand. Therefore, it could be beneficial to experiment with different window sizes and evaluate their impact on the performance of the model.
For categorical data, I proposed finding frequent temporal patterns. A pattern might be as simple as two consecutive instances of a particular category or more complex combinations of category sequences. This approach is known as categorical abstraction. By identifying frequent patterns, I can capture relationships between different categories that might be relevant for prediction tasks.
For temporal data, I focused on creating temporal features, which are features that change over time. This is particularly relevant in the context of the "Quantified Self" domain, where many of the tracked variables are time series.
In addition to these basic feature engineering techniques, I also used more advanced techniques like Fourier Transformations and text data pre-processing. Fourier Transformations allow me to represent any sequence of measurements as a combination of sinusoidal functions with differing frequencies. This can be particularly beneficial when dealing with periodic or repetitive data, like running patterns. Text data pre-processing, on the other hand, is useful for handling unstructured data like text. It involves steps like tokenization, casing, stemming, and stop word removal, and can be followed by techniques like Bag of Words and TF-IDF to extract features from the text.
The resulting set of features was extensive and diverse, capturing a wide range of information from the original data. This is beneficial because it gives the machine learning model a lot of information to learn from. However, it also presents a challenge in terms of dimensionality. With so many features, the model might suffer from the curse of dimensionality, where the high-dimensional feature space becomes sparse and the model struggles to learn. To mitigate this, I can use techniques like feature selection or dimensionality reduction to reduce the number of features while retaining the most informative ones.
Incorporating the techniques from Chapter 5 of "Machine Learning for the Quantified Self", I also applied clustering techniques to the engineered features. Clustering is a technique that groups similar instances together. In the context of my project, I used clustering to find structure in the data,which could be used to create new features or modify existing ones. For instance, I could identify clusters of similar activity instances based on the accelerometer, linear acceleration, and magnetometer readings, and use these clusters as categorical features in my model. The clustering was performed using both non-hierarchical (k-means) and hierarchical methods, depending on the specific requirements of the data and the problem at hand. The number of clusters (k) was determined based on the data and was provided as a command-line argument to the script. The script also provided the option to choose the clustering method (k-means, k-medoids, agglomerative, or final mode) via command-line arguments, allowing for flexibility and experimentation with different clustering techniques.
Finally, Principal Component Analysis (PCA) was applied to the dataset. PCA is a technique used to reduce the dimensionality of the data, and it was applied with 7 components, as this was found to explain most of the variance in the data. The number of principal components was determined by examining the explained variance ratio of the components and choosing a number of components that captured a significant portion of the variance without adding unnecessary complexity to the model. However, the optimal number of components can depend on various factors, including the complexity of the data and the model, and the trade-off between model simplicity and performance. Therefore, it could be beneficial to experiment with different numbers of components and evaluate their impact on the performance of the model.
Transition to Model Building
Having engineered the features, I was then ready to move onto the next stage of the process: model building. This stage involves the application of machine learning algorithms to the engineered features to create a predictive model. The transition from feature engineering to model building represents a shift from preparing and shaping the data to actively learning patterns from it to serve my research question: "Can I predict the activity being performed based on measurements from the smartphone's accelerometer, linear acceleration, and magnetometer?" This transition is a crucial part of the process, as it sets the stage for the application of machine learning techniques to my cleaned, preprocessed, and feature-engineered data.
Critical Reflection
Reflecting critically on the entire process so far, from data collection and cleaning to feature engineering, it's clear that each stage has its own challenges and limitations. In the data collection and cleaning stage, I had to make decisions about outlier detection and missing value imputation, which could have a significant impact on the quality of the data. In the feature engineering stage, I had to make decisions about the appropriate techniques to use for each data type and the parameters for these techniques, which could impact the quality of the features and, consequently, the performance of the machine learning models. These challenges highlight the importance of a thorough and thoughtful approach to each stage of the process, as well as the need for continuous critical reflection and evaluation. For instance, the choice of a window size of 30 for moving window statistics was based on the nature of my data and the activities I was trying to predict. However, a different window size might have yielded different results. Similarly, the decision to retain seven principal components was based on the explained variance ratio, but a different number of components might have been more appropriate depending on the complexity of the data and the model. These reflections highlight the importance of a thorough and thoughtful approach to each stage of the process, as well as the need for continuous critical reflection and evaluation.
Machine Learning Approaches
Defining an Appropriate Train/Test Setup
The first step in the machine learning process was to divide the dataset into a training set and a test set. This is a crucial step in any machine learning project as it allows us to evaluate the performance of our models on unseen data, providing an estimate of how well the model will perform in real-world scenarios.
The dataset was split into a training set and a test set using stratified sampling. Stratified sampling ensures that the training and test sets have similar distributions of the target variable, which in this case is the activity label. This is important because it ensures that our model is trained and evaluated on a representative sample of the data. The split was set at 70% for the training set and 30% for the test set, a common ratio that provides a good balance between having enough data to train the model and enough data to test it.
Application of Classical Machine Learning Techniques
Once the train/test setup was defined, the next step was to apply classical machine learning techniques to the dataset. The techniques used included a feedforward neural network, a decision tree, a random forest, and a support vector machine with a kernel. These models were chosen because they are versatile, powerful, and well-suited to the task of activity recognition.
The feedforward neural network is a type of artificial neural network where the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any), and to the output nodes. It is capable of modeling complex, non-linear relationships, which makes it a good choice for this task.
The decision tree and random forest models are simple yet powerful machine learning models that work well for a wide range of tasks. They are particularly good at handling categorical data and can model non-linear relationships. The random forest model, which is an ensemble of decision trees, can also help to reduce overfitting, which can be a problem with decision trees.
The support vector machine with a kernel is a powerful model that can handle both linear and non-linear data. It works by transforming the data into a higher-dimensional space where it can be separated by a hyperplane.
Hyperparameter Optimization
After training the models, the next step was to optimize their hyperparameters. Hyperparameters are parameters that are not learned from the data but are set before the learning process begins. They control the learning process and can have a significant impact on the performance of the model.
The hyperparameters were optimized using a grid search, which is a simple and widely used method for hyperparameter tuning. It works by defining a grid of hyperparameters and then evaluating the performance of the model for each combination of hyperparameters in the grid.
For each model, a range of possible values was defined for each hyperparameter. The grid search then trained and evaluated the model for each combination of hyperparameters, and the combination that resulted in the best performance was selected.
Results and Reflection
The results of the machine learning models were evaluated using several metrics, including accuracy, precision, recall, and F1 score. These metrics provide a comprehensive view of the model's performance, taking into account both the true positives and the false positives, as well as the true negatives and the false negatives.
Reflecting on the process, the choice of machine learning models and the method for hyperparameter tuning were appropriate for the task at hand. However, there is always room for improvement. For instance, other models or ensemble methods could be explored, and more advanced methods for hyperparameter tuning, such as random search or Bayesian optimization, could be used. Furthermore, the performance of the models could potentially be improved by further feature engineering or by using a larger or more diverse dataset.


 
 
 
 
Feature Engineering: Numerical, Categorical, and Temporal Data
After cleaning and preprocessing the data, I transitioned to the next stage of the process: feature engineering. This stage involves the creation of new features or the modification of existing features in the dataset to improve machine learning model performance. The goal is to provide the model with the most relevant information in a format that it can understand and learn from. The transition from data cleaning to feature engineering represents a shift from preparing the data to actively shaping it to best serve my research question: "Can I predict the activity being performed based on measurements from the smartphone's accelerometer, linear acceleration, and magnetometer?" This transition is a crucial part of the process, as it sets the stage for the application of machine learning techniques to my cleaned and preprocessed data.
In the context of my research, which involves tracking personal data generated by my own behavioral activities, I have a variety of data types, including numerical, categorical, and temporal data. Each of these data types requires a different approach to feature engineering.
For numerical data, I used a window of time to summarize the data over that window with various measures such as the mean, median, minimum, maximum, standard deviation, or slope. This approach is known as numerical abstraction. By summarizing the data over a window of time, I can capture trends and patterns in the data that might be relevant for prediction tasks. The choice of a window size of 30 was made based on the nature of my data and the activities I was trying to predict. A window size of 30 seconds was deemed sufficient to capture the relevant patterns in the data without introducing too much noise. However, it's worth noting that the optimal window size can vary depending on the specific characteristics of the data and the problem at hand. Therefore, it could be beneficial to experiment with different window sizes and evaluate their impact on the performance of the model.
For categorical data, I proposed finding frequent temporal patterns. A pattern might be as simple as two consecutive instances of a particular category or more complex combinations of category sequences. This approach is known as categorical abstraction. By identifying frequent patterns, I can capture relationships between different categories that might be relevant for prediction tasks.
For temporal data, I focused on creating temporal features, which are features that change over time. This is particularly relevant in the context of the "Quantified Self" domain, where many of the tracked variables are time series.
In addition to these basic feature engineering techniques, I also used more advanced techniques like Fourier Transformations and text data pre-processing. Fourier Transformations allow me to represent any sequence of measurements as a combination of sinusoidal functions with differing frequencies. This can be particularly beneficial when dealing with periodic or repetitive data, like running patterns. Text data pre-processing, on the other hand, is useful for handling unstructured data like text. It involves steps like tokenization, casing, stemming, and stop word removal, and can be followed by techniques like Bag of Words and TF-IDF to extract features from the text.
The resulting set of features was extensive and diverse, capturing a wide range of information from the original data. This is beneficial because it gives the machine learning model a lot of information to learn from. However, it also presents a challenge in terms of dimensionality. With so many features, the model might suffer from the curse of dimensionality, where the high-dimensional feature space becomes sparse and the model struggles to learn. To mitigate this, I can use techniques like feature selection or dimensionality reduction to reduce the number of features while retaining the most informative ones.
Finally, Principal Component Analysis (PCA) was applied to the dataset. PCA is a technique used to reduce the dimensionality of the data, and it was applied with 7 components, as this was found toexplain most of the variance in the data. The number of principal components was determined by examining the explained variance ratio of the components and choosing a number of components that captured a significant portion of the variance without adding unnecessary complexity to the model. However, the optimal number of components can depend on various factors, including the complexity of the data and the model, and the trade-off between model simplicity and performance. Therefore, it could be beneficial to experiment with different numbers of components and evaluate their impact on the performance of the model.
Transition to Model Building
Having engineered the features, I was then ready to move onto the next stage of the process: model building. This stage involves the application of machine learning algorithms to the engineered features to create a predictive model. The transition from feature engineering to model building represents a shift from preparing and shaping the data to actively learning patterns from it to serve my research question: "Can I predict the activity being performed based on measurements from the smartphone's accelerometer, linear acceleration, and magnetometer?" This transition is a crucial part of the process, as it sets the stage for the application of machine learning techniques to my cleaned, preprocessed, and feature-engineered data.
Critical Reflection
Reflecting critically on the entire process so far, from data collection and cleaning to feature engineering, it's clear that each stage has its own challenges and limitations. In the data collection and cleaning stage, I had to make decisions about outlier detection and missing value imputation, which could have a significant impact on the quality of the data. In the feature engineering stage, I had to make decisions about the appropriate techniques to use for each data type and the parameters for these techniques, which could impact the quality of the features and, consequently, the performance of the machine learning models. These challenges highlight the importance of a thorough and thoughtful approach to each stage of the process, as well as the need for continuous critical reflection and evaluation. For instance, the choice of a window size of 30 for moving window statistics was based on the nature of my data and the activities I was trying to predict. However, a different window size might have yielded different results. Similarly, the decision to retain seven principal components was based on the explained variance ratio, but a different number of components might have been more appropriate depending on the complexity of the data and the model. These reflections highlight the importance of a thorough and thoughtful approach to each stage of the process, as well as the need for continuous critical reflection and evaluation.
Defining an Appropriate Train/Test Setup
The first step in the machine learning process was to divide the dataset into a training set and a test set. This is a crucial step in any machine learning project as it allows us to evaluate the performance of our models on unseen data, providing an estimate of how well the model will perform in real-world scenarios.
The dataset was split into a training set and a test set using stratified sampling. Stratified sampling ensures that the training and test sets have similar distributions of the target variable, which in this case is the activity label. This is important because it ensures that our model is trained and evaluated on a representative sample of the data. The split was set at 70% for the training set and 30% for the test set, a common ratio that provides a good balance between having enough data to train the model and enough data to test it.
Application of Classical Machine Learning Techniques
Once the train/test setup was defined, the next step was to apply classical machine learning techniques to the dataset. The techniques used included a feedforward neural network, a decision tree, a random forest, and a support vector machine with a kernel. These models were chosen because they are versatile, powerful, and well-suited to the task of activity recognition.
The feedforward neural network is a type of artificial neural network where the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any), and to the output nodes. It is capable of modeling complex, non-linear relationships, which makes it a good choice for this task.
The decision tree and random forest models are simple yet powerful machine learning models that work well for a wide range of tasks. They are particularly good at handling categorical data and can model non-linear relationships. The random forest model, which is an ensemble of decision trees, can also help to reduce overfitting, which can be a problem with decision trees.
The support vector machine with a kernel is a powerful model that can handle both linear and non-linear data. It works by transforming the data into a higher-dimensional space where it can be separated by a hyperplane.
Hyperparameter Optimization
After training the models, the next step was to optimize their hyperparameters. Hyperparameters are parameters that are not learned from the data but are set before the learning process begins. They control the learning process and can have a significant impact on the performance of the model.
The hyperparameters were optimized using a grid search, which is a simple and widely used method for hyperparameter tuning. It works by defining a grid of hyperparameters and then evaluating the performance of the model for each combination of hyperparameters in the grid.
For each model, a range of possible values was defined for each hyperparameter. The grid search then trained and evaluated the model for each combination of hyperparameters, and the combination that resulted in the best performance was selected.
Results and Reflection
The results of the machine learning models were evaluated using several metrics, including accuracy, precision, recall, and F1 score. These metrics provide a comprehensive view of the model's performance, taking into account both the true positives and the false positives, as well as the true negatives and the false negatives.
Reflecting on the process, the choice of machine learning models and the method for hyperparameter tuning were appropriate for the task at hand. However, there is always room for improvement. For instance, other models or ensemble methods could be explored, and more advanced methods for hyperparameter tuning, such as random search or Bayesian optimization, could be used. Furthermore, the performance of the models could potentially be improved by further feature engineering or by using a larger or more diverse dataset.

